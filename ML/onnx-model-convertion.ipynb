{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9c13603",
   "metadata": {},
   "source": [
    "# Optimize ONNX Model With TensorRT\n",
    "The scope of this notebook is convertion of well known network to highly optimize inference agent using TensorRT Python API with ONNX format network.\n",
    "\n",
    "To this end we will get classification network in ONNX format from the ONNX Zoo - https://github.com/onnx/models\n",
    "\n",
    "### Intro on ONNX\n",
    "ONNX (Open Neural Network Exchange) is an open format built to represent machine learning models. ONNX defines a common set of operators - the building blocks of machine learning and deep learning models - and a common file format to enable AI developers to use models with a variety of frameworks, tools, runtimes, and compilers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68608d0",
   "metadata": {},
   "source": [
    "### Setup\n",
    "* tf2onnx - python library to convert tensorflow models to ONNX format\n",
    "* onnx-simplifier - python library to help us simplify the generated ONNX descriptor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e84b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tf2onnx onnx-simplifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01eea413",
   "metadata": {},
   "source": [
    "Limit GPU memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5aa56ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only allocate 1GB of memory on the first GPU\n",
    "  try:\n",
    "    tf.config.experimental.set_virtual_device_configuration(\n",
    "        gpus[0],\n",
    "        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=2500)])\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Virtual devices must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8474fb",
   "metadata": {},
   "source": [
    "### Model Creation\n",
    "Fisrt we import the dataset and preprocess it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a2d4003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done converting to float32 normalized values\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.keras.utils.np_utils import to_categorical\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "def load_dataset():\n",
    "    # load dataset\n",
    "    (trainX, trainY), (testX, testY) = mnist.load_data()\n",
    "    # reshape dataset to have a single channel\n",
    "    trainX = trainX.reshape((trainX.shape[0], 28, 28, 1))\n",
    "    testX = testX.reshape((testX.shape[0], 28, 28, 1))\n",
    "    # one hot encode target values\n",
    "    trainY = to_categorical(trainY)\n",
    "    testY = to_categorical(testY)\n",
    "    return trainX, trainY, testX, testY\n",
    "\n",
    "\n",
    "x_train, y_train, x_test, y_test = load_dataset()\n",
    "\n",
    "def prep_pixels(train, test):\n",
    "    # convert from integers to floats\n",
    "    train_norm = train.astype('float32')\n",
    "    test_norm = test.astype('float32')\n",
    "    # normalize to range 0-1\n",
    "    train_norm = train_norm / 255.0\n",
    "    test_norm = test_norm / 255.0\n",
    "    # return normalized images\n",
    "    return train_norm, test_norm\n",
    "\n",
    "\n",
    "x_train, x_test = prep_pixels(x_train, x_test)\n",
    "print(\"Done converting to float32 normalized values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb892874",
   "metadata": {},
   "source": [
    "Define the model and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a6c5b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1875/1875 [==============================] - 17s 9ms/step - loss: 0.1729 - accuracy: 0.9461\n",
      "Epoch 2/2\n",
      "1875/1875 [==============================] - 17s 9ms/step - loss: 0.0604 - accuracy: 0.9816\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc7625d9ee0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=(28, 28, 1)),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(100, activation='relu', kernel_initializer='he_uniform'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "opt = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9)\n",
    "model.compile(optimizer=opt, loss=tf.keras.losses.CategoricalCrossentropy(), metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e05d3d",
   "metadata": {},
   "source": [
    "Run inference on the current model and measure time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d2481bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "s1 = time.time()\n",
    "np.argmax(model.predict(np.asanyarray([x_test[0]]),), axis=-1)\n",
    "e1 = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3246747e",
   "metadata": {},
   "source": [
    "### Convert to ONNX\n",
    "To convert to ONNX format file we use tf2onnx python lib, it parses to model api from tensorflow and encode the data into ONNX format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e6b4088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tf2onnx/tf_loader.py:603: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
      "Saved onnx model to /tmp/model.onnx\n"
     ]
    }
   ],
   "source": [
    "import tf2onnx\n",
    "\n",
    "input_signature = [tf.TensorSpec([1, 28, 28, 1], tf.float32, name='x')]\n",
    "onnx_model, _ = tf2onnx.convert.from_keras(model, input_signature, opset=13)\n",
    "with open(\"/tmp/model.onnx\", \"wb\") as f:\n",
    "    f.write(onnx_model.SerializeToString())\n",
    "\n",
    "print(\"Saved onnx model to /tmp/model.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00881429",
   "metadata": {},
   "source": [
    "tf2onnx requires the input shape of the model, and the desired ONNX format version (opset - can learn here: https://github.com/onnx/onnx/blob/master/docs/Versioning.md)\n",
    "\n",
    "After conversion we save the onnx object to binary file, which later can be used to load the model to other frameworks / tensorrt\n",
    "\n",
    "It is recommended to use onnx simplification on the output model, often, especially in large models, the output of the tf2onnx exporter (or any other exporter for that matter) in a complicated description of the network. we can use onnx-simplify python lib to simplify the output model.\n",
    "\n",
    "e.g. Before simplification\n",
    "\n",
    "<img src='images/complex.png' height=\"50%\" width=\"50%\">\n",
    "\n",
    "After simplification\n",
    "\n",
    "<img src='images/simple.png' height=\"15%\" width=\"15%\">\n",
    "\n",
    "This is recommended because this is a known voodoo that TensorRT does not handle well with big networks complex description.\n",
    "\n",
    "#### Simplify the ONNX model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d614ecc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved onnx model to /tmp/model-simplified.onnx\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "from onnxsim import simplify\n",
    "\n",
    "# load your predefined ONNX model\n",
    "model = onnx.load('/tmp/model.onnx')\n",
    "\n",
    "# convert model\n",
    "model_simp, check = simplify(model)\n",
    "\n",
    "# save simplified model\n",
    "with open(\"/tmp/model-simplified.onnx\", \"wb\") as f:\n",
    "    f.write(onnx_model.SerializeToString())\n",
    "    \n",
    "print(\"Saved onnx model to /tmp/model-simplified.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb56254",
   "metadata": {},
   "source": [
    "### Convert ONNX Model to TensorRT Engine\n",
    "\n",
    "To convert the ONNX model we can use three methods:\n",
    "* trtexec cli utility\n",
    "* TensorRT Python API\n",
    "* TensorRT C++ API\n",
    "\n",
    "We're going to show the trtexec method and the python cli method.\n",
    "\n",
    "#### Convert using trtexec to engine file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "70d6645c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "&&&& RUNNING TensorRT.trtexec # trtexec --onnx=/tmp/model-simplified.onnx --explicitBatch --workspace=64 --buildOnly --saveEngine=optimized.trt\n",
      "[06/07/2021-17:55:38] [I] === Model Options ===\n",
      "[06/07/2021-17:55:38] [I] Format: ONNX\n",
      "[06/07/2021-17:55:38] [I] Model: /tmp/model-simplified.onnx\n",
      "[06/07/2021-17:55:38] [I] Output:\n",
      "[06/07/2021-17:55:38] [I] === Build Options ===\n",
      "[06/07/2021-17:55:38] [I] Max batch: explicit\n",
      "[06/07/2021-17:55:38] [I] Workspace: 64 MiB\n",
      "[06/07/2021-17:55:38] [I] minTiming: 1\n",
      "[06/07/2021-17:55:38] [I] avgTiming: 8\n",
      "[06/07/2021-17:55:38] [I] Precision: FP32\n",
      "[06/07/2021-17:55:38] [I] Calibration: \n",
      "[06/07/2021-17:55:38] [I] Refit: Disabled\n",
      "[06/07/2021-17:55:38] [I] Safe mode: Disabled\n",
      "[06/07/2021-17:55:38] [I] Save engine: optimized.trt\n",
      "[06/07/2021-17:55:38] [I] Load engine: \n",
      "[06/07/2021-17:55:38] [I] Builder Cache: Enabled\n",
      "[06/07/2021-17:55:38] [I] NVTX verbosity: 0\n",
      "[06/07/2021-17:55:38] [I] Tactic sources: Using default tactic sources\n",
      "[06/07/2021-17:55:38] [I] Input(s)s format: fp32:CHW\n",
      "[06/07/2021-17:55:38] [I] Output(s)s format: fp32:CHW\n",
      "[06/07/2021-17:55:38] [I] Input build shapes: model\n",
      "[06/07/2021-17:55:38] [I] Input calibration shapes: model\n",
      "[06/07/2021-17:55:38] [I] === System Options ===\n",
      "[06/07/2021-17:55:38] [I] Device: 0\n",
      "[06/07/2021-17:55:38] [I] DLACore: \n",
      "[06/07/2021-17:55:38] [I] Plugins:\n",
      "[06/07/2021-17:55:38] [I] === Inference Options ===\n",
      "[06/07/2021-17:55:38] [I] Batch: Explicit\n",
      "[06/07/2021-17:55:38] [I] Input inference shapes: model\n",
      "[06/07/2021-17:55:38] [I] Iterations: 10\n",
      "[06/07/2021-17:55:38] [I] Duration: 3s (+ 200ms warm up)\n",
      "[06/07/2021-17:55:38] [I] Sleep time: 0ms\n",
      "[06/07/2021-17:55:38] [I] Streams: 1\n",
      "[06/07/2021-17:55:38] [I] ExposeDMA: Disabled\n",
      "[06/07/2021-17:55:38] [I] Data transfers: Enabled\n",
      "[06/07/2021-17:55:38] [I] Spin-wait: Disabled\n",
      "[06/07/2021-17:55:38] [I] Multithreading: Disabled\n",
      "[06/07/2021-17:55:38] [I] CUDA Graph: Disabled\n",
      "[06/07/2021-17:55:38] [I] Separate profiling: Disabled\n",
      "[06/07/2021-17:55:38] [I] Skip inference: Enabled\n",
      "[06/07/2021-17:55:38] [I] Inputs:\n",
      "[06/07/2021-17:55:38] [I] === Reporting Options ===\n",
      "[06/07/2021-17:55:38] [I] Verbose: Disabled\n",
      "[06/07/2021-17:55:38] [I] Averages: 10 inferences\n",
      "[06/07/2021-17:55:38] [I] Percentile: 99\n",
      "[06/07/2021-17:55:38] [I] Dump refittable layers:Disabled\n",
      "[06/07/2021-17:55:38] [I] Dump output: Disabled\n",
      "[06/07/2021-17:55:38] [I] Profile: Disabled\n",
      "[06/07/2021-17:55:38] [I] Export timing to JSON file: \n",
      "[06/07/2021-17:55:38] [I] Export output to JSON file: \n",
      "[06/07/2021-17:55:38] [I] Export profile to JSON file: \n",
      "[06/07/2021-17:55:38] [I] \n",
      "[06/07/2021-17:55:38] [I] === Device Information ===\n",
      "[06/07/2021-17:55:38] [I] Selected Device: Tesla K80\n",
      "[06/07/2021-17:55:38] [I] Compute Capability: 3.7\n",
      "[06/07/2021-17:55:38] [I] SMs: 13\n",
      "[06/07/2021-17:55:38] [I] Compute Clock Rate: 0.8235 GHz\n",
      "[06/07/2021-17:55:38] [I] Device Global Memory: 11441 MiB\n",
      "[06/07/2021-17:55:38] [I] Shared Memory per SM: 112 KiB\n",
      "[06/07/2021-17:55:38] [I] Memory Bus Width: 384 bits (ECC enabled)\n",
      "[06/07/2021-17:55:38] [I] Memory Clock Rate: 2.505 GHz\n",
      "[06/07/2021-17:55:38] [I] \n",
      "[06/07/2021-17:55:38] [I] [TRT] ----------------------------------------------------------------\n",
      "[06/07/2021-17:55:38] [I] [TRT] Input filename:   /tmp/model-simplified.onnx\n",
      "[06/07/2021-17:55:38] [I] [TRT] ONNX IR version:  0.0.7\n",
      "[06/07/2021-17:55:38] [I] [TRT] Opset version:    13\n",
      "[06/07/2021-17:55:38] [I] [TRT] Producer name:    tf2onnx\n",
      "[06/07/2021-17:55:38] [I] [TRT] Producer version: 1.8.5\n",
      "[06/07/2021-17:55:38] [I] [TRT] Domain:           \n",
      "[06/07/2021-17:55:38] [I] [TRT] Model version:    0\n",
      "[06/07/2021-17:55:38] [I] [TRT] Doc string:       \n",
      "[06/07/2021-17:55:38] [I] [TRT] ----------------------------------------------------------------\n",
      "[06/07/2021-17:55:38] [W] [TRT] /home/jenkins/agent/workspace/OSS/OSS_L0_MergeRequest/oss/parsers/onnx/onnx2trt_utils.cpp:227: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.\n",
      "[06/07/2021-17:55:38] [W] [TRT] /home/jenkins/agent/workspace/OSS/OSS_L0_MergeRequest/oss/parsers/onnx/builtin_op_importers.cpp:1322: Weight sequential/dense/MatMul/ReadVariableOp:0 has been transposed! If you plan on overwriting this weight with the Refitter API, the new weights must be pre-transposed\n",
      "[06/07/2021-17:55:38] [W] [TRT] /home/jenkins/agent/workspace/OSS/OSS_L0_MergeRequest/oss/parsers/onnx/builtin_op_importers.cpp:1322: Weight sequential/dense_1/MatMul/ReadVariableOp:0 has been transposed! If you plan on overwriting this weight with the Refitter API, the new weights must be pre-transposed\n",
      "[06/07/2021-17:55:41] [I] [TRT] Some tactics do not have sufficient workspace memory to run. Increasing workspace size may increase performance, please check verbose output.\n",
      "[06/07/2021-17:55:41] [I] [TRT] Detected 1 inputs and 1 output network tensors.\n",
      "[06/07/2021-17:55:41] [I] Engine built in 3.3178 sec.\n",
      "&&&& PASSED TensorRT.trtexec # trtexec --onnx=/tmp/model-simplified.onnx --explicitBatch --workspace=64 --buildOnly --saveEngine=optimized.trt\n"
     ]
    }
   ],
   "source": [
    "!trtexec --onnx=/tmp/model-simplified.onnx --explicitBatch --workspace=64 --buildOnly --saveEngine=optimized.trt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13dc4c1",
   "metadata": {},
   "source": [
    "#### Convert onnx model to TensorRT engine using Python API\n",
    "\n",
    "> Note: TensorRT can't handle at the moment with dynamic batch size, which means the most common issue with ONNX model optimization is forgetting to set the EXPLICIT_BATCH flag in the NetworkDefinitionCreationFlag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1e08c811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trt version 7.2.3.4\n",
      "Parsed ONNX successfully\n",
      "building tensorrt engine\n",
      "Saving serialized model\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "import tensorrt as trt\n",
    "\n",
    "logger = trt.Logger(trt.Logger.VERBOSE)\n",
    "logger.min_severity = trt.Logger.Severity.VERBOSE\n",
    "EXPLICIT_BATCH = []\n",
    "\n",
    "print('trt version', trt.__version__)\n",
    "if trt.__version__[0] >= '7':\n",
    "    EXPLICIT_BATCH.append(1 << (int)(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))\n",
    "\n",
    "with trt.Builder(logger) as builder, builder.create_network(*EXPLICIT_BATCH) as network, trt.OnnxParser(network, logger) as parser:\n",
    "    builder.max_workspace_size = 1 << 28\n",
    "    builder.max_batch_size = 1\n",
    "    builder.fp16_mode = False\n",
    "\n",
    "    with open('/tmp/model-simplified.onnx', 'rb') as f:\n",
    "        if not parser.parse(f.read()):\n",
    "            for error in range(parser.num_errors):\n",
    "                print(parser.get_error(error))\n",
    "        else:\n",
    "            print(\"Parsed ONNX successfully\")\n",
    "\n",
    "    # reshape input from 32 to 1\n",
    "    shape = list(network.get_input(0).shape)\n",
    "    print(\"building tensorrt engine\")\n",
    "    engine = builder.build_cuda_engine(network)\n",
    "    print(\"Saving serialized model\")\n",
    "    with open('optimized.trt', 'wb') as f:\n",
    "        f.write(engine.serialize())\n",
    "        \n",
    "    print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12e6300",
   "metadata": {},
   "source": [
    "### Inference of TensorRT compile engine\n",
    "\n",
    "We define here some utility functions to load and allocate memory to the inputs and outputs of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "13e4226e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorrt as trt\n",
    "import pycuda.autoinit\n",
    "import pycuda.driver as cuda\n",
    "\n",
    "\n",
    "logger = trt.Logger(trt.Logger.VERBOSE)\n",
    "logger.min_severity = trt.Logger.Severity.VERBOSE\n",
    "\n",
    "class HostDeviceMem(object):\n",
    "    def __init__(self, host_mem, device_mem):\n",
    "        \"\"\"Within this context, host_mom means the cpu memory and device means the GPU memory\n",
    "        \"\"\"\n",
    "        self.host = host_mem \n",
    "        self.device = device_mem\n",
    "    def __str__(self):\n",
    "        return \"Host:\\n\" + str(self.host) + \"\\nDevice:\\n\" + str(self.device)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "    \n",
    "\n",
    "def allocate_buffers(engine):\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    bindings = []\n",
    "    stream = cuda.Stream()\n",
    "    for binding in engine:\n",
    "        size = trt.volume(engine.get_binding_shape(binding)) * engine.max_batch_size\n",
    "        dtype = trt.nptype(engine.get_binding_dtype(binding))\n",
    "        # Allocate host and device buffers\n",
    "        host_mem = cuda.pagelocked_empty(size, dtype)\n",
    "        device_mem = cuda.mem_alloc(host_mem.nbytes)\n",
    "        # Append the device buffer to device bindings.\n",
    "        bindings.append(int(device_mem))\n",
    "        # Append to the appropriate list.\n",
    "        if engine.binding_is_input(binding):\n",
    "            inputs.append(HostDeviceMem(host_mem, device_mem))\n",
    "        else:\n",
    "            outputs.append(HostDeviceMem(host_mem, device_mem))\n",
    "    return inputs, outputs, bindings, stream\n",
    "\n",
    "def load_engine(engine_path):\n",
    "    with open(engine_path, \"rb\") as f, trt.Runtime(logger) as runtime:\n",
    "        return runtime.deserialize_cuda_engine(f.read())\n",
    "\n",
    "engine = load_engine('./optimized.trt')\n",
    "\n",
    "ctx = engine.create_execution_context() \n",
    "\n",
    "# Allocate buffers for input and output\n",
    "inputs, outputs, bindings, stream = allocate_buffers(engine) # input, output: host # bindings \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "35b738cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def do_inference(context, bindings, inputs, outputs, stream, batch_size=1):\n",
    "    # Transfer data from CPU to the GPU.\n",
    "    [cuda.memcpy_htod_async(inp.device, inp.host, stream) for inp in inputs]\n",
    "    # Run inference.\n",
    "    context.execute_async(batch_size=batch_size, bindings=bindings, stream_handle=stream.handle)\n",
    "    # Transfer predictions back from the GPU.\n",
    "    [cuda.memcpy_dtoh_async(out.host, out.device, stream) for out in outputs]\n",
    "    # Synchronize the stream\n",
    "    stream.synchronize()\n",
    "    # Return only the host outputs.\n",
    "    return [out.host for out in outputs]\n",
    "\n",
    "def postprocess_the_outputs(h_outputs, shape_of_output):\n",
    "    h_outputs = h_outputs.reshape(*shape_of_output)\n",
    "    return h_outputs\n",
    "\n",
    "max_batch_size = 1 # The batch size of input mush be smaller the max_batch_size once the engine is built\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4f9fc3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.copyto(inputs[0].host, np.asanyarray(x_train[0]).ravel())\n",
    "\n",
    "shape_of_output = (1, 10)\n",
    "s2 = time.time()\n",
    "np.argmax(postprocess_the_outputs(do_inference(ctx, bindings, inputs, outputs, stream)[0], shape_of_output))\n",
    "e2 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "67829db2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model: 0.09577703475952148\n",
      "Optimized model: 0.0016007423400878906\n"
     ]
    }
   ],
   "source": [
    "print(\"Original model: \" + str(e1 - s1))\n",
    "print(\"Optimized model: \" + str(e2 - s2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
