{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I4a8-V9kMqi5"
   },
   "source": [
    "# Regression problem with Deep Learning (Keras)\n",
    "\n",
    "\n",
    "In this task, we are going to build a neural network which will predict the house of a price, based on some relevant parameters.\n",
    "\n",
    "At a high-level, neural networks are either encoders, decoders, or a combination of both. Encoders find patterns in raw data to form compact, useful representations. Decoders generate new data or high-resolution useful infomation from those representations. \n",
    "\n",
    "As a reminder, **Feed Forward Neural Networks (FFNNs)** is a network which its goal is to work on classification and regression problems, based on features (X->Y mapping, supervised problem). See the [aspect](#Part-1:-Boston-Housing-Price-Prediction-with-Feed-Forward-Neural-Networks) of this notebook for an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E7zLv-j5Mqi6"
   },
   "source": [
    "## Part 0: Prerequisites:\n",
    "\n",
    "\n",
    "[tf.keras](https://www.tensorflow.org/guide/keras) is the simplest way to build and train neural network models in TensorFlow (w/ Keras).\n",
    "\n",
    "Note that there's [tf.keras](https://www.tensorflow.org/guide/keras) (comes with TensorFlow) and there's [Keras](https://keras.io/) (standalone). You should be using [tf.keras](https://www.tensorflow.org/guide/keras) because (1) it comes with TensorFlow so you don't need to install anything extra and (2) it comes with powerful TensorFlow-specific features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "0-W-w9bzMqi7",
    "outputId": "9065afe6-07a1-4c32-c9b5-b16394faa92d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-13 16:38:25.183895: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-03-13 16:38:25.183937: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.0\n"
     ]
    }
   ],
   "source": [
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling2D, Dropout, Flatten, Dense\n",
    "\n",
    "# Commonly used modules\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Images, plots, display, and visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "svVPdhSMMqi8"
   },
   "source": [
    "## Part 1: Boston Housing Price Prediction with Feed Forward Neural Networks\n",
    "\n",
    "Let's start with using a fully-connected neural network to do predict housing prices. The following image highlights the difference between regression and classification (see part 2). Given an observation as input, **regression** outputs a continuous value (e.g., exact temperature) and classificaiton outputs a class/category that the observation belongs to.\n",
    "\n",
    "<img src=\"https://i.imgur.com/vvSoAzg.jpg\" alt=\"classification_regression\" width=\"400\"/>\n",
    "\n",
    "For the Boston housing dataset, we get 506 rows of data, with 13 features in each. Our task is to build a regression model that takes these 13 features as input and output a single value prediction of the \"median value of owner-occupied homes (in thousands of USD).\"\n",
    "\n",
    "Now, we load the dataset. Loading the dataset returns four NumPy arrays:\n",
    "\n",
    "* The `train_images` and `train_labels` arrays are the *training set*—the data the model uses to learn.\n",
    "* The model is tested against the *test set*, the `test_images`, and `test_labels` arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "xADfD0B9Mqi9",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: X:  (404, 13)\n",
      "Train dataset size: y:  (404,)\n",
      "Test dataset size: X:  (102, 13)\n",
      "Test dataset size: y:  (102,)\n"
     ]
    }
   ],
   "source": [
    "(train_features, train_labels), (test_features, test_labels) = keras.datasets.boston_housing.load_data()\n",
    "\n",
    "# get per-feature statistics (mean, standard deviation) from the training set to normalize by\n",
    "train_mean = np.mean(train_features, axis=0)\n",
    "train_std = np.std(train_features, axis=0)\n",
    "train_features = (train_features - train_mean) / train_std\n",
    "print(\"Train dataset size: X: \", train_features.shape)\n",
    "print(\"Train dataset size: y: \", train_labels.shape)\n",
    "\n",
    "print(\"Test dataset size: X: \", test_features.shape)\n",
    "print(\"Test dataset size: y: \", test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.27224633, -0.48361547, -0.43576161, ...,  1.14850044,\n",
       "         0.44807713,  0.8252202 ],\n",
       "       [-0.40342651,  2.99178419, -1.33391162, ..., -1.71818909,\n",
       "         0.43190599, -1.32920239],\n",
       "       [ 0.1249402 , -0.48361547,  1.0283258 , ...,  0.78447637,\n",
       "         0.22061726, -1.30850006],\n",
       "       ...,\n",
       "       [-0.40202987,  0.99079651, -0.7415148 , ..., -0.71712291,\n",
       "         0.07943894, -0.67776904],\n",
       "       [-0.17292018, -0.48361547,  1.24588095, ..., -1.71818909,\n",
       "        -0.98764362,  0.42083466],\n",
       "       [-0.40422614,  2.04394792, -1.20161456, ..., -1.30866202,\n",
       "         0.23317118, -1.15392266]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8o4o8HnpMqi-"
   },
   "source": [
    "### Build the model\n",
    "\n",
    "Building the neural network requires configuring the layers of the model, then compiling the model. First we stack a few layers together using `keras.Sequential`. <br>\n",
    "Next we configure the loss function, optimizer, and metrics to monitor. These are added during the model's compile step:\n",
    "\n",
    "* *Loss function* - measures how accurate the model is during training, we want to minimize this with the optimizer.\n",
    "* *Optimizer* - how the model is updated based on the data it sees and its loss function.\n",
    "* *Metrics* - used to monitor the training and testing steps.\n",
    "\n",
    "Your goal is to build a network that is getting input as the input size, has 3 hidden layers (each one with a different size), and one output layer (as we want to have 1 number at the end).\n",
    "Also, loss function should be: mse (mean squared error), optimizer should be Adam, and we should use mae + mse as our metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "hQVUVJs8Mqi_"
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = keras.Sequential([\n",
    "        Dense(20, activation=tf.nn.relu, input_shape=[len(train_features[0])]),\n",
    "        Dense(20, activation=tf.nn.relu, input_shape=[20]),\n",
    "        Dense(20, activation=tf.nn.relu, input_shape=[20]),\n",
    "        Dense(1)\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=tf.optimizers.Adam(), \n",
    "                  loss='mse',\n",
    "                  metrics=['mae', 'mse'])\n",
    "    return model\n",
    "\n",
    "def build_cnn_model():\n",
    "    model = keras.Sequential()\n",
    "    model.add(Conv1D(32, (2), activation='relu', input_shape=[len(train_features[0])]))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Conv1D(64, (2), activation='relu'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Conv1D(64, (2), activation='relu')),\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    model = keras.Sequential(optimizer = 'adam',\n",
    "                             loss=tf.keras.losses.MeanSquaredError(reduction=\"auto\", name=\"mean_squared_error\"), metrics=['mae', 'mse'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qoo9SMfnMqi_"
   },
   "source": [
    "### Train the model\n",
    "\n",
    "Training the neural network model requires the following steps:\n",
    "\n",
    "1. Feed the training data to the model—in this example, the `train_features` and `train_labels` arrays.\n",
    "2. The model learns to associate features and labels.\n",
    "3. We ask the model to make predictions about a test set—in this example, the `test_features` array. We verify that the predictions match the labels from the `test_labels` array. \n",
    "\n",
    "To start training,  call the `model.fit` method—the model is \"fit\" to the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "lt0aIIX1MqjA",
    "outputId": "7c245880-2d27-4111-b21d-6a29601cc971"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer \"conv1d_1\" is incompatible with the layer: expected min_ndim=3, found ndim=2. Full shape received: (None, 13)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# model instantiation        \u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_cnn_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# TODO: Read about EarlyStoppingRound!\u001b[39;00m\n\u001b[1;32m     13\u001b[0m early_stop \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mEarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m)\n",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36mbuild_cnn_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbuild_cnn_model\u001b[39m():\n\u001b[1;32m     15\u001b[0m     model \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mSequential()\n\u001b[0;32m---> 16\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mConv1D\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrelu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_features\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     model\u001b[38;5;241m.\u001b[39madd(MaxPooling2D((\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m)))\n\u001b[1;32m     18\u001b[0m     model\u001b[38;5;241m.\u001b[39madd(Conv1D(\u001b[38;5;241m64\u001b[39m, (\u001b[38;5;241m3\u001b[39m), activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[0;32m/opt/tljh/user/lib/python3.9/site-packages/tensorflow/python/training/tracking/base.py:530\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 530\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    532\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m previous_value  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[0;32m/opt/tljh/user/lib/python3.9/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/opt/tljh/user/lib/python3.9/site-packages/keras/engine/input_spec.py:227\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    225\u001b[0m   ndim \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;241m.\u001b[39mrank\n\u001b[1;32m    226\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m ndim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m ndim \u001b[38;5;241m<\u001b[39m spec\u001b[38;5;241m.\u001b[39mmin_ndim:\n\u001b[0;32m--> 227\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    228\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis incompatible with the layer: \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    229\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexpected min_ndim=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspec\u001b[38;5;241m.\u001b[39mmin_ndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    230\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfound ndim=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    231\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFull shape received: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtuple\u001b[39m(shape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    232\u001b[0m \u001b[38;5;66;03m# Check dtype.\u001b[39;00m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: Input 0 of layer \"conv1d_1\" is incompatible with the layer: expected min_ndim=3, found ndim=2. Full shape received: (None, 13)"
     ]
    }
   ],
   "source": [
    "# this helps makes our output less verbose but still shows progress\n",
    "# Here and above you have a full example of a NN model performance plotting\n",
    "# You should replicate this mechanism and apply it to other models as well (see TODO below)\n",
    "class PrintDot(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        if epoch % 100 == 0: print('')\n",
    "        print('.', end='')\n",
    "\n",
    "# model instantiation        \n",
    "model = build_cnn_model()\n",
    "\n",
    "# TODO: Read about EarlyStoppingRound!\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=50)\n",
    "\n",
    "# training the model and storing data into history\n",
    "history = model.fit(train_features, train_labels, epochs=1000, verbose=0, validation_split = 0.1,\n",
    "                    callbacks=[early_stop, PrintDot()])\n",
    "\n",
    "# using Pandas, generating data\n",
    "hist = pd.DataFrame(history.history)\n",
    "# Adding a column called epoch to that DataFrame\n",
    "hist['epoch'] = history.epoch\n",
    "hist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# RMSE \n",
    "### TODO: Calculate RMSE by the val_mse column\n",
    "rmse_final = np.sqrt((hist.val_mse - hist.mse) ** 2) / len(hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rmse_final)\n",
    "rmse_final = np.sum(rmse_final)\n",
    "print(rmse_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print()\n",
    "print('Final Mean Square Error on validation set: {}'.format(round(rmse_final, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SVgeQFjYMqjA"
   },
   "source": [
    "Now, let's plot the loss function measure on the training and validation sets. The validation set is used to prevent overfitting ([learn more about it here](https://www.tensorflow.org/tutorials/keras/overfit_and_underfit)). However, because our network is small, the training convergence without noticeably overfitting the data as the plot shows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ukh_98mYMqjA",
    "outputId": "eba47e56-640b-407c-c703-0ff3c1b1768d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Here and above you have a full example of a NN model performance plotting\n",
    "# You should replicate this mechanism and apply it to other models as well (see TODO below)\n",
    "def plot_history():\n",
    "    plt.figure()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Mean Square Error [Thousand Dollars$^2$]')\n",
    "    plt.plot(hist['epoch'], hist['mse'], label='Train Error')\n",
    "    plt.plot(hist['epoch'], hist['val_mse'], label = 'Val Error')\n",
    "    plt.legend()\n",
    "    plt.ylim([0,50])\n",
    "\n",
    "plot_history()\n",
    "\n",
    "\n",
    "### TODO: Make sure to have the same flow for CNN as well, as the goal will be to see 2 plots:\n",
    "# each one contain train error and val error on each model (NN, CNN) respectively"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8P5ZfvEIMqjB"
   },
   "source": [
    "Next, compare how the model performs on the test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZijCEOIzMqjB",
    "outputId": "acb0c62b-d96c-4aec-f9de-f855f9e9e0b6"
   },
   "outputs": [],
   "source": [
    "# No need to touch\n",
    "test_features_norm = (test_features - train_mean) / train_std\n",
    "mse, _, _ = model.evaluate(test_features_norm, test_labels)\n",
    "rmse = np.sqrt(mse)\n",
    "print('Root Mean Square Error on test set: {}'.format(round(rmse, 3)))\n",
    "\n",
    "## TODO: Make sure to have a mechanism for CNN as well, as above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZFph_wLwMqjG"
   },
   "source": [
    "## Acknowledgements\n",
    "\n",
    "The contents of this tutorial is based on and inspired by the work of [TensorFlow team](https://www.tensorflow.org) (see their [Colab notebooks](https://www.tensorflow.org/tutorials/)), our [MIT Human-Centered AI team](https://hcai.mit.edu), and individual pieces referenced in the [MIT Deep Learning](https://deeplearning.mit.edu) course slides."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Done! You just finished a competition between NN and CNN, where both network knows how to predict house prices"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "tutorial_deep_learning_basics.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
