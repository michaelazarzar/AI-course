{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41aec9bb",
   "metadata": {},
   "source": [
    "<h2> Getting into Neural Networks! </h2> \n",
    "<h3> Ex. 1 </h3>\n",
    "<br>\n",
    "The goal of this excercise is to get you into two main pieces in NN:\n",
    "<ul>\n",
    "    <li>Activation Functions</li>\n",
    "    <li>Neurons (aka Perceptrons aka functions)</li>\n",
    "</ul>\n",
    "For this part, you will use solely NumPy!\n",
    "<br>\n",
    "Goal: \n",
    "<li> (1) Go over the activation main functions and understand their logic </li>\n",
    "<li> (2) Implement a neuron which gets an input, does a transformation and produces a single ouput on top of it \n",
    "</li>\n",
    "<li> (3) Implement tanh using the formula which can easily found online, not using that built it function </li>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a142950",
   "metadata": {},
   "source": [
    "Let's read the functions of Sigmoid, TanH, ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bff2dff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Relu, Sigmoid, Tanh methods\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def sigmoid(X):\n",
    "    \"\"\"Implements sigmoid given x\"\"\"\n",
    "    return 1/(1+np.exp(-X))\n",
    "\n",
    "def tanh(x):\n",
    "    \"\"\"Implements hyperbolic tan, given (x)\"\"\"\n",
    "    return np.tanh(x) # Btw, using this implementation, we can see that some functions are built in\n",
    "\n",
    "def relu(x):\n",
    "    \"\"\"ReLU function gets a value x and checks if it's lower than 0, hence returns 0, else returns the value x itself\"\"\"\n",
    "    return np.maximum(x, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65414a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODOs:\n",
    "# 1. Implement tanh without the np.tanh build-in function, and check yourselves against it, using the below inputs:\n",
    "# 1. [1, 6, 9], [-1, 6, -10], and on x1 = 1, x2 = 3\n",
    "# 2. Given the below inputs: x1 = 1, x2 = 2, write a function that gets these inputs from and returns a number, which will be determined by all 3 options below:\n",
    "# Meaning: if sigmoid => value will be calculated by sigmoid\n",
    "         # if tanh => value will be calculated by tanh\n",
    "         # if relu => value will be calculated by relu\n",
    "# Assuming weights can be any static value, same for the bias, the formula should follow what we saw in the presentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6872ba03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid: 0.9933071490757153\n",
      "tanh: 0.9999092042625951\n",
      "relu: 5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x = 5\n",
    "print(f'sigmoid: {sigmoid(x)}')\n",
    "print(f'tanh: {tanh(x)}')\n",
    "print(f'relu: {relu(x)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3af8939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_tanh: 0.9999092042625951\n"
     ]
    }
   ],
   "source": [
    "def my_tanh(x):\n",
    "    e_power = math.e ** (2*x)\n",
    "    return (e_power - 1) / (e_power + 1)\n",
    "\n",
    "\n",
    "print(f'my_tanh: {my_tanh(x)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e799359f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid: 0.7310585786300049\n",
      "sigmoid: 0.7310585786300049\n",
      "relu: 1.0\n",
      "relu: 1.0\n",
      "tanh: 0.7615941559557649\n",
      "tanh: 0.7615941559557649\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ws = [[1 , 6, 9] , [-1 , 6, 10]]\n",
    "activations = ['sigmoid' , 'relu', 'tanh']\n",
    "for activation in activations:\n",
    "    for w in ws:\n",
    "\n",
    "        def my_tanh1(weights, x1, x2):\n",
    "            w0 = weights[0]\n",
    "            w1 = weights[1] \n",
    "            w2 = weights[2]\n",
    "            return tanh(w0 + w1 * x1 + w2 * x2)\n",
    "    \n",
    "        z = my_tanh1(w, 1,2)\n",
    "        if (activation == 'sigmoid'):\n",
    "            print(f'sigmoid: {sigmoid(z)}')\n",
    "        elif (activation == 'relu'):\n",
    "            print(f'relu: {relu(z)}')\n",
    "        else :\n",
    "            print(f'tanh: {tanh(z)}')\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d9833f0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ba003f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
