{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QJc0KMZlzdtO"
   },
   "source": [
    "# Welcome!\n",
    "In this tutorial we'll walk through a simple convolutional neural network to classify the images in the Simpson dataset using Keras.\n",
    "\n",
    "We’ll also set up Weights & Biases to log models metrics, inspect performance and share findings about the best architecture for the network. In this example we're using Google Colab as a convenient hosted environment, but you can run your own training scripts from anywhere and visualize metrics with W&B's experiment tracking tool.\n",
    "\n",
    "### Running This Notebook\n",
    "1. Click \"Open in playground\" to create a copy of this notebook for yourself.\n",
    "2. Save a copy in Google Drive for yourself.\n",
    "3. To enable a GPU, please click Edit > Notebook Settings. Change the \"hardware accelerator\" to GPU.\n",
    "4. Step through each section below, pressing play on the code blocks to run the cells.\n",
    "\n",
    "Results will be logged to a [shared W&B project page](https://app.wandb.ai/wandb/keras-intro).\n",
    "\n",
    "We highly encourage you to fork this notebook, tweak the parameters, or try the model with your own dataset!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fVXNBu8Q68r5"
   },
   "source": [
    "# Setup\n",
    "Start out by installing the experiment tracking library and setting up your free W&B account:\n",
    "\n",
    "\n",
    "*   **pip install wandb** – Install the W&B library\n",
    "*   **import wandb** – Import the wandb library\n",
    "*   **from wandb.keras import WandbCallback** – Import the wandb [keras callback](https://docs.wandb.com/library/frameworks/keras)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting opencv-python\n",
      "  Using cached opencv_python-4.5.5.64-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (60.5 MB)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /opt/tljh/user/lib/python3.9/site-packages (from opencv-python) (1.22.0)\n",
      "Installing collected packages: opencv-python\n",
      "Successfully installed opencv-python-4.5.5.64\n",
      "Found existing installation: opencv-python 4.5.5.64\n",
      "Uninstalling opencv-python-4.5.5.64:\n",
      "  Successfully uninstalled opencv-python-4.5.5.64\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: opencv-python-headless==4.5.3.56 in /home/jupyter-e4-michaelazarzar/.local/lib/python3.9/site-packages (4.5.3.56)\n",
      "Requirement already satisfied: numpy>=1.19.3 in /opt/tljh/user/lib/python3.9/site-packages (from opencv-python-headless==4.5.3.56) (1.22.0)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install wandb -q\n",
    "!pip install opencv-python\n",
    "!python -m pip uninstall opencv-python --yes\n",
    "!pip install opencv-python-headless==4.5.3.56"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Lxjw5Qckzg5W",
    "outputId": "0d762b25-d621-4cdd-aa37-271a52d20e62"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-27 11:08:02.664887: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-03-27 11:08:02.664952: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "# WandB – Install the W&B library\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "id": "DMMDN6JraJ5S"
   },
   "outputs": [],
   "source": [
    "# Essentials\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import train_test_split, learning_curve, KFold, cross_val_score, StratifiedKFold\n",
    "\n",
    "# Models\n",
    "import tensorflow as tf\n",
    "\n",
    "# Image Libraries\n",
    "from PIL import Image, ImageFilter, ImageStat\n",
    "import imageio, glob, cv2\n",
    "import os\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import AxesGrid\n",
    "import random\n",
    "random.seed(42)\n",
    "%matplotlib inline\n",
    "\n",
    "# Ignore excessive warnings\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XD-LaKjpaJ5a"
   },
   "source": [
    "### Explore The Simpsons Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "prsXWfw8b9Pr",
    "outputId": "59ab1e0b-aac8-4644-9c70-535beb9f0d42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'simpsons-dataset' already exists and is not an empty directory.\r\n"
     ]
    }
   ],
   "source": [
    "# Fetch the dataset form Github\n",
    "!git clone https://github.com/lavanyashukla/simpsons-dataset.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 592
    },
    "id": "Bemz_9lLaJ5b",
    "outputId": "1f3b1b2a-9080-4163-8075-4519e0b7b602"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'cv2' has no attribute 'imread'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m i \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m character \u001b[38;5;129;01min\u001b[39;00m characters[:\u001b[38;5;241m25\u001b[39m]:\n\u001b[0;32m----> 7\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimread\u001b[49m(character)\n\u001b[1;32m      8\u001b[0m     img \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mresize(img, (\u001b[38;5;241m250\u001b[39m, \u001b[38;5;241m250\u001b[39m))\n\u001b[1;32m      9\u001b[0m     plt\u001b[38;5;241m.\u001b[39maxis(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moff\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'cv2' has no attribute 'imread'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 720x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize images in the dataset\n",
    "characters = glob.glob('simpsons-dataset/kaggle_simpson_testset/kaggle_simpson_testset/**')\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "i = 0\n",
    "for character in characters[:25]:\n",
    "    img = cv2.imread(character)\n",
    "    img = cv2.resize(img, (250, 250))\n",
    "    plt.axis('off')\n",
    "    plt.subplot(5, 5, i+1) #.set_title(l)\n",
    "    plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ijwd2qbG2LWK"
   },
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LphhtX-0aJ5d"
   },
   "outputs": [],
   "source": [
    "# Define the labels for the Simpsons characters we're detecting\n",
    "character_names = {0: 'abraham_grampa_simpson', 1: 'apu_nahasapeemapetilon', 2: 'bart_simpson', \n",
    "        3: 'charles_montgomery_burns', 4: 'chief_wiggum', 5: 'comic_book_guy', 6: 'edna_krabappel', \n",
    "        7: 'homer_simpson', 8: 'kent_brockman', 9: 'krusty_the_clown', 10: 'lenny_leonard', 11:'lisa_simpson',\n",
    "        12: 'marge_simpson', 13: 'mayor_quimby',14:'milhouse_van_houten', 15: 'moe_szyslak', \n",
    "        16: 'ned_flanders', 17: 'nelson_muntz', 18: 'principal_skinner', 19: 'sideshow_bob'}\n",
    "img_size = 64\n",
    "num_classes = 20\n",
    "dir = \"simpsons-dataset/simpsons_dataset/simpsons_dataset\"\n",
    "\n",
    "# Load training data\n",
    "X_train = []\n",
    "y_train = []\n",
    "for label, name in character_names.items():\n",
    "   list_images = os.listdir(dir+'/'+name)\n",
    "   for image_name in list_images:\n",
    "       image = imageio.imread(dir+'/'+name+'/'+image_name)\n",
    "       X_train.append(cv2.resize(image, (img_size,img_size)))\n",
    "       y_train.append(label)\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "# Split data for cross validation  \n",
    "X_train = X_train[:10000] \n",
    "y_train = y_train[:10000]\n",
    "\n",
    "X_test = X_train[-100:] \n",
    "y_test = y_train[-100:]\n",
    "\n",
    "# Normalize the data\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "# One hot encode the labels (neural nets only like numbers)\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FT4dKp1xVeeT"
   },
   "source": [
    "# Training A Simple Neural Network\n",
    "\n",
    "### Define Your Hyperparameters\n",
    "*   **wandb.init()** – Initialize a new W&B run. Each run is single execution of the training script.\n",
    "*   **wandb.config** – Save all your hyperparameters in a config object. This lets you use W&B app to sort and compare your runs by hyperparameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "id": "ue4Xg1b65Vr4",
    "outputId": "91332d9e-0285-43bd-ab78-3d274384c664"
   },
   "outputs": [],
   "source": [
    "# Initilize a new wandb run\n",
    "wandb.init(entity=\"michaelazarzar\", project=\"keras-intro-michael\")\n",
    "\n",
    "# Default values for hyper-parameters\n",
    "config = wandb.config # Config is a variable that holds and saves hyperparameters and inputs\n",
    "config.learning_rate = 0.01\n",
    "config.epochs = 1000\n",
    "config.img_width=28\n",
    "config.img_height=28\n",
    "config.num_classes = 10\n",
    "config.batch_size = 128\n",
    "config.validation_size = 5000\n",
    "config.weight_decay = 0.0005\n",
    "config.activation = 'relu'\n",
    "config.optimizer = 'nadam'\n",
    "config.seed = 42\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eWfsGAOF5edM"
   },
   "source": [
    "### Define Your Neural Network\n",
    "Below, we define a simplified version of a VGG19 model in Keras, and add the following lines of code to log models metrics, visualize performance and output and track our experiments easily:\n",
    "*   **callbacks=[WandbCallback()]** – Fetch all layer dimensions, model parameters and log them automatically to your W&B dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 546
    },
    "id": "xuylXt5m5ghy",
    "outputId": "d8640eee-c388-4964-af0c-3d8c640eb7cc"
   },
   "outputs": [],
   "source": [
    "%%wandb\n",
    "# Determine input shape\n",
    "input_shape = (X_train.shape[1], X_train.shape[2], 3)\n",
    "\n",
    "# Define the model architecture - This is a simplified version of the VGG19 architecture\n",
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "# Set of Conv2D, Conv2D, MaxPooling2D layers with 32 and 64 filters\n",
    "# Conv2D adds a convolution layer that generates 2 dimensional feature maps to learn different aspects of our image\n",
    "model.add(tf.keras.layers.Conv2D(filters = 32, kernel_size = (3, 3), padding = 'same', \n",
    "                  activation ='relu', input_shape = input_shape))\n",
    "\n",
    "# MaxPooling2D layer reduces the size of the image representation our convolutional layers learnt,\n",
    "# and in doing so it reduces the number of parameters and computations the network needs to perform.\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Flattens our array so we can feed the convolution layer outputs (a matrix) into our fully connected layer (an array)\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "\n",
    "# Dense layer creates dense, fully connected layers with x inputs and y outputs - it simply outputs the dot product of our inputs and weights\n",
    "model.add(tf.keras.layers.Dense(512, activation ='relu'))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(num_classes, activation = \"softmax\"))\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = tf.keras.optimizers.Nadam(lr=config.learning_rate, beta_1=0.9, beta_2=0.999, clipnorm=1.0)\n",
    "\n",
    "model.compile(loss = \"categorical_crossentropy\", optimizer = optimizer, metrics=['accuracy'])\n",
    "\n",
    "# Data augmentation\n",
    "datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "    samplewise_center=False,  # set each sample mean to 0\n",
    "    featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "    samplewise_std_normalization=False,  # divide each input by its std\n",
    "    zca_whitening=False,  # apply ZCA whitening\n",
    "    rotation_range=15,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "    width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "    height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "    horizontal_flip=True,  # randomly flip images\n",
    "    vertical_flip=False)  # randomly flip images\n",
    "# (std, mean, and principal components if ZCA whitening is applied).\n",
    "datagen.fit(X_train)\n",
    "\n",
    "# Fit the model to the training data\n",
    "model.fit_generator(datagen.flow(X_train, y_train, batch_size=config.batch_size),\n",
    "                    steps_per_epoch=len(X_train) / 32, epochs=config.epochs,\n",
    "                    validation_data=(X_test, y_test), verbose=0,\n",
    "                    callbacks=[WandbCallback(data_type=\"image\", validation_data=(X_test, y_test), labels=character_names),\n",
    "                                tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LWGT7TSS3cYx"
   },
   "source": [
    "### Make Predictions\n",
    "In this section we make predictions and add wandb.log() to log custom images - in this case our test images with predicted probabilities overlaid on top.\n",
    "\n",
    "*   **wandb.log()** – Logs custom objects – these can be images, videos, audio files, HTML, plots, point clouds etc. Here we use wandb.log to log images of Simpson characters overlaid with actual and predicted labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gUaHHRYo3cuo"
   },
   "outputs": [],
   "source": [
    "def make_predictions(nn_model):\n",
    "  predicted_images = []\n",
    "  for i in range(20):\n",
    "    character = character_names[i]\n",
    "    # Read in a character image from the test dataset\n",
    "    image = cv2.imread(np.random.choice([k for k in glob.glob('simpsons-dataset/kaggle_simpson_testset/kaggle_simpson_testset/*.*') if character in k]))\n",
    "    img = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Resize image and normalize it\n",
    "    pic = cv2.resize(image, (64, 64)).astype('float32') / 255.\n",
    "    \n",
    "    # Get predictions for the character\n",
    "    prediction = nn_model.predict(pic.reshape(1, 64, 64,3))[0]\n",
    "    \n",
    "    # Get true name of the character\n",
    "    name = character.split('_')[0].title()\n",
    "    \n",
    "    # Format predictions to string to overlay on image\n",
    "    text = sorted(['{:s} : {:.1f}%'.format(character_names[k].split('_')[0].title(), 100*v) for k,v in enumerate(prediction)], \n",
    "        key=lambda x:float(x.split(':')[1].split('%')[0]), reverse=True)[:3]\n",
    "    \n",
    "    # Upscale image\n",
    "    img = cv2.resize(img, (352, 352))\n",
    "    \n",
    "    # Create background to overlay text on\n",
    "    cv2.rectangle(img, (0,260),(215,352),(255,255,255), -1)\n",
    "    \n",
    "    # Add text to image -  We add the true probabilities and predicted probabilities on each of the images in the test dataset\n",
    "    font = cv2.FONT_HERSHEY_DUPLEX\n",
    "    cv2.putText(img, 'True Name : %s' % name, (10, 280), font, 0.7,(73,79,183), 2, cv2.LINE_AA)\n",
    "    for k, t in enumerate(text):\n",
    "        cv2.putText(img, t, (10, 300+k*18), font, 0.65,(0,0,0), 2, cv2.LINE_AA)\n",
    "        \n",
    "    # Add predicted image from test dataset with annotations to array\n",
    "    predicted_images.append(wandb.Image(img, caption=\"Actual: %s\" % name))     \n",
    "        \n",
    "  # Log images from test set to wandb automatically, along with predicted and true labels by passing pytorch tensors with image data into wandb.Image\n",
    "  # You can use wandb.log() to log any images, video, audio, 3D objects like point clouds, plots, HTML etc.\n",
    "  # For full details please see https://docs.wandb.com/library/python/log\n",
    "  wandb.log({\"predictions\": predicted_images})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Z8Gzk6n5vw0"
   },
   "outputs": [],
   "source": [
    "make_predictions(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xftEwKyuaJ5q"
   },
   "source": [
    "# Visualize Predictions Live\n",
    "\n",
    "## Project Overview\n",
    "1. Check out the [project page](https://app.wandb.ai/wandb/keras-intro) to see your results in the shared project. \n",
    "1. Press 'option+space' to expand the runs table, comparing all the results from everyone who has tried this script. \n",
    "1. Click on the name of a run to dive in deeper to that single run on its own run page.\n",
    "\n",
    "![](https://paper-attachments.dropbox.com/s_92F7A2BE132D5E4492B0E3FF3430FFF0FB2390A4135C0D77582A2D21A2EF8567_1574108018540_Screenshot+2019-11-18+12.13.29.png)\n",
    "\n",
    "\n",
    "## Visualize Performance\n",
    "Click through to a single run to see more details about that run. For example, on [this run page](https://app.wandb.ai/wandb/keras-intro/runs/tjbbvw5o) you can see the performance metrics I logged when I ran this script.\n",
    "\n",
    "![](https://paper-attachments.dropbox.com/s_92F7A2BE132D5E4492B0E3FF3430FFF0FB2390A4135C0D77582A2D21A2EF8567_1574108139591_Screenshot+2019-11-18+12.14.50.png)\n",
    "\n",
    "\n",
    "## Review Code\n",
    "The overview tab picks up a link to the code. In this case, it's a link to the Google Colab. If you're running a script from a git repo, we'll pick up the SHA of the latest git commit and give you a link to that version of the code in your own GitHub repo.\n",
    "\n",
    "![](https://paper-attachments.dropbox.com/s_92F7A2BE132D5E4492B0E3FF3430FFF0FB2390A4135C0D77582A2D21A2EF8567_1574108018524_Screenshot+2019-11-18+12.13.14.png)\n",
    "\n",
    "## Visualize System Metrics\n",
    "The System tab on the runs page lets you visualize how resource efficient your model was. It lets you monitor the GPU, memory, CPU, disk, and network usage in one spot.\n",
    "\n",
    "![](https://paper-attachments.dropbox.com/s_92F7A2BE132D5E4492B0E3FF3430FFF0FB2390A4135C0D77582A2D21A2EF8567_1574107906110_Screenshot+2019-11-18+12.11.39.png)\n",
    "\n",
    "\n",
    "## Next Steps\n",
    "As you can see running sweeps is super easy! We highly encourage you to fork this notebook, tweak the parameters, or try the model with your own dataset!\n",
    "\n",
    "\n",
    "## More about Weights & Biases\n",
    "We're always free for academics and open source projects. Email carey@wandb.com with any questions or feature suggestions. Here are some more resources:\n",
    "\n",
    "1. [Documentation](http://docs.wandb.com) - Python docs\n",
    "2. [Gallery](https://app.wandb.ai/gallery) - example reports in W&B\n",
    "3. [Articles](https://www.wandb.com/articles) - blog posts and tutorials\n",
    "4. [Community](bit.ly/wandb-forum) - join our Slack community forum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "96rvZKfAi2Kc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Intro to Keras with W&B.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
