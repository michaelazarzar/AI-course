{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4307447b",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39d5a3f",
   "metadata": {},
   "source": [
    "<p style=\"line-height:1.75;font-size:16px\">\n",
    "As we've talked about earlier, researchers spend a lot of their time processing and preparing the data prior to modeling. In most cases, this process is lengthy, tedious and non-trivial but it has a very big impact on our final results and therefore needs to be addressed.<br>\n",
    "In this notebook, we will talk about various techiques of dealing with common issues that you may encounter in your work. This is by no means a comprehensive guide of all the possible issues and solutions but rather a set of tools that you can add to the ones we've already talked about earlier (e.g. data scaling)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2397f3",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Handling Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8d454e",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### What is it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1593dda2",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<p style=\"line-height:1.75;font-size:16px\">\n",
    "Missing values are probably the most common issues you'll encounter when dealing with data. As the name suggests, i merely means that certain features in our dataset have no values for some or all records. There are many reasons why values can be missing from a dataset - thess reasons might also affect the way we choose to handle these missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3abb6d",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Why is it an issue?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2126a2ed",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<p style=\"line-height:1.75;font-size:16px\">\n",
    "From a technical perspective, some models do not know how to deal with missing (n/a, nan) values and will just return an error when we try to give them data with missing values. Even if the model does know how to deal with missing values, we must still ask ourselves - should we let it?<Br> When it boils down to it, a model needs a numerical value to work with so it will need to assign <u>some</u> value to the missing value. The number 0 is most likely to be assigned but it doesn't always makes sense. For instance, what if we have a feature measuring a person's height in cm, having a value of 0 is probably not a good idea.<br>  \n",
    "It is far better to have control of how we fill in these missing values rather then letting the model choose an arbitrary value which might not make a lot of sense."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39ae980",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Possible solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aacb90c",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Dropping rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf51f66",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<p style=\"line-height:1.75;font-size:16px\">\n",
    "If row (record) is missing a lot of values or missing a specific value that is critical for our prediction, we might choose to drop this row altogether. Using this solution might lead to a large loss of data so we would usually choose it only after we've exhausted our other options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d4e8429",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embarked Column\n",
      "Dataset before: 891\n",
      "Missing values: 2\n",
      "Dataset after: 889\n",
      "\n",
      "\n",
      "Any Column\n",
      "Dataset before: 891\n",
      "Dataset after: 183\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Drop all rows with missing data in 'Embarked' column\n",
    "print('Embarked Column')\n",
    "df = pd.read_csv('data/titanic.csv')\n",
    "print('Dataset before:', len(df))\n",
    "print('Missing values:', df['Embarked'].isnull().sum())\n",
    "df.dropna(subset=['Embarked'], how='any', inplace=True)\n",
    "print('Dataset after:', len(df))\n",
    "print('\\n')\n",
    "\n",
    "# Drop all rows with missing data in any column\n",
    "print('Any Column')\n",
    "df = pd.read_csv('data/titanic.csv')\n",
    "print('Dataset before:', len(df))\n",
    "df.dropna(how='any', inplace=True)\n",
    "print('Dataset after:', len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1653f1a8",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Dropping columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ccb26bb",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<p style=\"line-height:1.75;font-size:16px\">\n",
    "If a column (feature) has many missing values it usually makes sense to drop the column. Although we could, in theory, impute the data (more on that soon) when too much of it is missing, imputation will not work well and therefore dropping the column completely is probably a better idea.<br>\n",
    "It's important to note that if the column we're considering dropping is crucial for our analysis we won't be able to use this solution and instead we'll have to think of how we can fill in this data (e.g. collect more samples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f5aad6a2",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset before: 891\n",
      "Missing values: 687\n",
      "Missing values (%): 77.10437710437711\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Drop a column with many missing values\n",
    "df = pd.read_csv('data/titanic.csv')\n",
    "print('Dataset before:', len(df))\n",
    "print('Missing values:', df['Cabin'].isnull().sum())\n",
    "print('Missing values (%):', 100 * df['Cabin'].isnull().sum()/len(df))\n",
    "df.drop('Cabin', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d432d3",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f24dac1",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<p style=\"line-height:1.75;font-size:16px\">\n",
    "Imputation is the process of replacing missing data with substituted values. This is usually done by looking at other non-missing values of a feature and choosing a method to infer the missing values. A common method to impute missing numerical values is to use the mean or median. In the case of categorical features, the mode is often used.<br>\n",
    "Imputation introduces synthetic samples into our data and should be used with caution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d0062daf",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean\n",
      "Missing Age: 140, Average Age: 29.498846153846156\n",
      "Missing Age: 0, Average Age: 29.498846153846152\n",
      "\n",
      "\n",
      "Constant\n",
      "Missing Age: 140, Average Age: 29.498846153846156\n",
      "Missing Age: 0, Average Age: 29.794016853932586\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Imputing single numerical column using mean\n",
    "print('Mean')\n",
    "df = pd.read_csv('data/titanic.csv')\n",
    "train, test = train_test_split(df, train_size=0.8, random_state=42)\n",
    "print(f'Missing Age: {train[\"Age\"].isnull().sum()}, Average Age: {np.mean(train[\"Age\"])}')\n",
    "simple_imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "train['Age'] = simple_imputer.fit_transform(train[['Age']])\n",
    "print(f'Missing Age: {train[\"Age\"].isnull().sum()}, Average Age: {np.mean(train[\"Age\"])}')\n",
    "print('\\n')\n",
    "\n",
    "# Imputing single numerical column using constant\n",
    "print('Constant')\n",
    "df = pd.read_csv('data/titanic.csv')\n",
    "train, test = train_test_split(df, train_size=0.8, random_state=42)\n",
    "print(f'Missing Age: {train[\"Age\"].isnull().sum()}, Average Age: {np.mean(train[\"Age\"])}')\n",
    "simple_imputer = SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=31)\n",
    "train['Age'] = simple_imputer.fit_transform(train[['Age']])\n",
    "print(f'Missing Age: {train[\"Age\"].isnull().sum()}, Average Age: {np.mean(train[\"Age\"])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e90ce2f7",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean\n",
      "Missing BsmtQual: 28, Mode Value: TA\n",
      "Missing BsmtQual: 0, Mode Value: TA\n",
      "\n",
      "\n",
      "Constant\n",
      "Missing BsmtQual: 28, Unique Values: ['TA' 'Gd' 'Fa' nan 'Ex']\n",
      "Missing BsmtQual: 0, Unique Values: ['TA' 'Gd' 'Fa' 'UNKNOWN' 'Ex']\n"
     ]
    }
   ],
   "source": [
    "# Imputing single categorical column using mode\n",
    "print('Mean')\n",
    "df = pd.read_csv('data/iowa_housing_price.csv')\n",
    "train, test = train_test_split(df, train_size=0.8, random_state=42)\n",
    "print(f'Missing BsmtQual: {train[\"BsmtQual\"].isnull().sum()}, Mode Value: {train[\"BsmtQual\"].mode().values[0]}')\n",
    "simple_imputer = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "train['BsmtQual'] = simple_imputer.fit_transform(train[['BsmtQual']])\n",
    "print(f'Missing BsmtQual: {train[\"BsmtQual\"].isnull().sum()}, Mode Value: {train[\"BsmtQual\"].mode().values[0]}')\n",
    "print('\\n')\n",
    "\n",
    "# Imputing single categorical column using constant\n",
    "print('Constant')\n",
    "df = pd.read_csv('data/iowa_housing_price.csv')\n",
    "train, test = train_test_split(df, train_size=0.8, random_state=42)\n",
    "print(f'Missing BsmtQual: {train[\"BsmtQual\"].isnull().sum()}, Unique Values: {train[\"BsmtQual\"].unique()}')\n",
    "simple_imputer = SimpleImputer(missing_values=np.nan, strategy='constant', fill_value='UNKNOWN')\n",
    "train['BsmtQual'] = simple_imputer.fit_transform(train[['BsmtQual']])\n",
    "print(f'Missing BsmtQual: {train[\"BsmtQual\"].isnull().sum()}, Unique Values: {train[\"BsmtQual\"].unique()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c137ad7",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<p style=\"line-height:1.75;font-size:16px\">\n",
    "    Although the <code>SimpleImputer</code> is the one most commonly used, we could also impute values using a more complex imputer called <code>KNNImputer</code>. This imputer looks for k similar rows, without missing values for the feature we are trying to impute, and calculates the missing values as the average (or weighted average) of that feature in the most similar rows - let's see how it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b41ca9e6",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Age: 140, Average Age: 29.498846153846156\n",
      "Missing Age: 0, Average Age: 29.805252808988765\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "df = pd.read_csv('data/titanic.csv')\n",
    "df.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1, inplace=True)\n",
    "train, test = train_test_split(df, train_size=0.8, random_state=42)\n",
    "train = pd.get_dummies(train)\n",
    "print(f'Missing Age: {train[\"Age\"].isnull().sum()}, Average Age: {np.mean(train[\"Age\"])}')\n",
    "knn_imputer = KNNImputer(missing_values=np.nan, n_neighbors=5, weights='uniform')\n",
    "train[train.columns] = knn_imputer.fit_transform(train)\n",
    "print(f'Missing Age: {train[\"Age\"].isnull().sum()}, Average Age: {np.mean(train[\"Age\"])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75fa19f7",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Handling Categorical Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5dd2e6",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### What is it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bf09e7",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<p style=\"line-height:1.75;font-size:16px\">\n",
    "A categorical feature is one that only takes a finite number of distinct values. These values often have an underlying meaning and can't simply be treated as number. Moreover, in many cases, these categorical features are formatted as text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7268f9",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Why is it an issue?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbae94b",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<p style=\"line-height:1.75;font-size:16px\">\n",
    "We know that machine learning models work with numbers under the hood and as such they rely on various numerical properties (e.g. 4 > 3). However, with categorical variables, the numerical properties don't always hold and this might introduce noise to our data and confuse the mode. For instance, think of a categorical variable such as a zipcode - 40013 is greater than 35593 but do we want our model to treat it a such? Probably not since we know that's not how zipcodes work.<br>\n",
    "    On the other hand, if we have a categorical feature such a shirt size which can be Small, Medium or Large, transforming it into an ordinal variable with the values 1, 2 and 3 might makes sense.<br>\n",
    "    Categorical variables should be dealt with on a case by case basis and there isn't one solution that fits all."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060564fb",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Possible Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990a1df8",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fc81a778",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 education\n",
      "12556             basic.6y\n",
      "35451          high.school\n",
      "30592  professional.course\n",
      "17914    university.degree\n",
      "3315              basic.4y\n",
      "29191             basic.4y\n",
      "30549             basic.9y\n",
      "6098     university.degree\n",
      "6252     university.degree\n",
      "13373              unknown\n",
      "       basic.4y  basic.6y  basic.9y  high.school  illiterate  \\\n",
      "12556       0.0       1.0       0.0          0.0         0.0   \n",
      "35451       0.0       0.0       0.0          1.0         0.0   \n",
      "30592       0.0       0.0       0.0          0.0         0.0   \n",
      "17914       0.0       0.0       0.0          0.0         0.0   \n",
      "3315        1.0       0.0       0.0          0.0         0.0   \n",
      "29191       1.0       0.0       0.0          0.0         0.0   \n",
      "30549       0.0       0.0       1.0          0.0         0.0   \n",
      "6098        0.0       0.0       0.0          0.0         0.0   \n",
      "6252        0.0       0.0       0.0          0.0         0.0   \n",
      "13373       0.0       0.0       0.0          0.0         0.0   \n",
      "\n",
      "       professional.course  university.degree  unknown            education  \n",
      "12556                  0.0                0.0      0.0             basic.6y  \n",
      "35451                  0.0                0.0      0.0          high.school  \n",
      "30592                  1.0                0.0      0.0  professional.course  \n",
      "17914                  0.0                1.0      0.0    university.degree  \n",
      "3315                   0.0                0.0      0.0             basic.4y  \n",
      "29191                  0.0                0.0      0.0             basic.4y  \n",
      "30549                  0.0                0.0      0.0             basic.9y  \n",
      "6098                   0.0                1.0      0.0    university.degree  \n",
      "6252                   0.0                1.0      0.0    university.degree  \n",
      "13373                  0.0                0.0      1.0              unknown  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv('data/banking.csv')\n",
    "train, test = train_test_split(df, train_size=0.8, random_state=42)\n",
    "\n",
    "print(train[['education']].head(10))\n",
    "one_hot_encoder = OneHotEncoder(sparse=False)\n",
    "train[one_hot_encoder.categories_[0]] = one_hot_encoder.fit_transform(train[['education']])\n",
    "print(train[list(one_hot_encoder.categories_[0]) + ['education']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ca4402",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Ordinal Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "517bf330",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 education\n",
      "12556             basic.6y\n",
      "35451          high.school\n",
      "30592  professional.course\n",
      "17914    university.degree\n",
      "3315              basic.4y\n",
      "29191             basic.4y\n",
      "30549             basic.9y\n",
      "6098     university.degree\n",
      "6252     university.degree\n",
      "13373              unknown\n",
      "       education\n",
      "12556        3.0\n",
      "35451        5.0\n",
      "30592        6.0\n",
      "17914        7.0\n",
      "3315         2.0\n",
      "29191        2.0\n",
      "30549        4.0\n",
      "6098         7.0\n",
      "6252         7.0\n",
      "13373        0.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv('data/banking.csv')\n",
    "train, test = train_test_split(df, train_size=0.8, random_state=42)\n",
    "\n",
    "print(train[['education']].head(10))\n",
    "ordinal_encoder = OrdinalEncoder(categories=[['unknown', 'illiterate', 'basic.4y', 'basic.6y', 'basic.9y', \n",
    "                                             'high.school', 'professional.course','university.degree']])\n",
    "train['education'] = ordinal_encoder.fit_transform(train[['education']])\n",
    "print(train[['education']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f535da3d",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['basic.4y', 'unknown', 'university.degree', 'high.school',\n",
       "       'basic.9y', 'professional.course', 'basic.6y', 'illiterate'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['education'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3fa5bba",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Hash Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f2ef2cad",
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting category_encoders\n",
      "  Downloading category_encoders-2.3.0-py2.py3-none-any.whl (82 kB)\n",
      "     |████████████████████████████████| 82 kB 565 kB/s             \n",
      "\u001b[?25hRequirement already satisfied: pandas>=0.21.1 in /opt/tljh/user/lib/python3.9/site-packages (from category_encoders) (1.3.5)\n",
      "Collecting patsy>=0.5.1\n",
      "  Downloading patsy-0.5.2-py2.py3-none-any.whl (233 kB)\n",
      "     |████████████████████████████████| 233 kB 30.0 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /opt/tljh/user/lib/python3.9/site-packages (from category_encoders) (1.22.0)\n",
      "Collecting statsmodels>=0.9.0\n",
      "  Downloading statsmodels-0.13.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.9 MB)\n",
      "     |████████████████████████████████| 9.9 MB 27.6 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: scipy>=1.0.0 in /opt/tljh/user/lib/python3.9/site-packages (from category_encoders) (1.7.3)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /opt/tljh/user/lib/python3.9/site-packages (from category_encoders) (1.0.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/tljh/user/lib/python3.9/site-packages (from pandas>=0.21.1->category_encoders) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/tljh/user/lib/python3.9/site-packages (from pandas>=0.21.1->category_encoders) (2.8.2)\n",
      "Requirement already satisfied: six in /opt/tljh/user/lib/python3.9/site-packages (from patsy>=0.5.1->category_encoders) (1.16.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/tljh/user/lib/python3.9/site-packages (from scikit-learn>=0.20.0->category_encoders) (3.0.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/tljh/user/lib/python3.9/site-packages (from scikit-learn>=0.20.0->category_encoders) (1.1.0)\n",
      "Installing collected packages: patsy, statsmodels, category-encoders\n",
      "Successfully installed category-encoders-2.3.0 patsy-0.5.2 statsmodels-0.13.1\n"
     ]
    }
   ],
   "source": [
    "!pip install -U category_encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bc4e18a6",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 education\n",
      "12556             basic.6y\n",
      "35451          high.school\n",
      "30592  professional.course\n",
      "17914    university.degree\n",
      "3315              basic.4y\n",
      "29191             basic.4y\n",
      "30549             basic.9y\n",
      "6098     university.degree\n",
      "6252     university.degree\n",
      "13373              unknown\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col_0</th>\n",
       "      <th>col_1</th>\n",
       "      <th>col_2</th>\n",
       "      <th>col_3</th>\n",
       "      <th>col_4</th>\n",
       "      <th>col_5</th>\n",
       "      <th>col_6</th>\n",
       "      <th>col_7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12556</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35451</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30592</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17914</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3315</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6265</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11284</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38158</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15795</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32950 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       col_0  col_1  col_2  col_3  col_4  col_5  col_6  col_7\n",
       "12556      0      0      0      0      0      0      1      0\n",
       "35451      0      0      0      0      0      1      0      0\n",
       "30592      0      0      0      1      0      0      0      0\n",
       "17914      0      0      1      0      0      0      0      0\n",
       "3315       0      0      0      1      0      0      0      0\n",
       "...      ...    ...    ...    ...    ...    ...    ...    ...\n",
       "6265       0      0      0      0      0      1      0      0\n",
       "11284      0      0      1      0      0      0      0      0\n",
       "38158      0      0      0      0      0      1      0      0\n",
       "860        0      0      0      1      0      0      0      0\n",
       "15795      0      0      1      0      0      0      0      0\n",
       "\n",
       "[32950 rows x 8 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from category_encoders import HashingEncoder\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv('data/banking.csv')\n",
    "train, test = train_test_split(df, train_size=0.8, random_state=42)\n",
    "\n",
    "print(train[['education']].head(10))\n",
    "hashing_encoder = HashingEncoder(n_components=8)\n",
    "hashing_encoder.fit_transform(train[['education']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d80d4a7",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Target Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1ebf78fe",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 education\n",
      "12556             basic.6y\n",
      "35451          high.school\n",
      "30592  professional.course\n",
      "17914    university.degree\n",
      "3315              basic.4y\n",
      "29191             basic.4y\n",
      "30549             basic.9y\n",
      "6098     university.degree\n",
      "6252     university.degree\n",
      "13373              unknown\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>education</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12556</th>\n",
       "      <td>0.081213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35451</th>\n",
       "      <td>0.107876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30592</th>\n",
       "      <td>0.112827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17914</th>\n",
       "      <td>0.136299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3315</th>\n",
       "      <td>0.103715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29191</th>\n",
       "      <td>0.103715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30549</th>\n",
       "      <td>0.078226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6098</th>\n",
       "      <td>0.136299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6252</th>\n",
       "      <td>0.136299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13373</th>\n",
       "      <td>0.141715</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       education\n",
       "12556   0.081213\n",
       "35451   0.107876\n",
       "30592   0.112827\n",
       "17914   0.136299\n",
       "3315    0.103715\n",
       "29191   0.103715\n",
       "30549   0.078226\n",
       "6098    0.136299\n",
       "6252    0.136299\n",
       "13373   0.141715"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from category_encoders import TargetEncoder\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv('data/banking.csv')\n",
    "train, test = train_test_split(df, train_size=0.8, random_state=42)\n",
    "\n",
    "print(train[['education']].head(10))\n",
    "target_encoder = TargetEncoder()\n",
    "target_encoder.fit_transform(train[['education']], train['y'])[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dded9c8",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
